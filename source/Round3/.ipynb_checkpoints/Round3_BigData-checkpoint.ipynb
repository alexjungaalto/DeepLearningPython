{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 3 - Machine Learning from Big Data \n",
    "\n",
    "Authors: Bhuwan Karki and Jyoti Prasad Bartaula \\\n",
    "Edited: Shamsi Abdurakhmanova\n",
    "\n",
    "So far we have applied ANNs to datasets that fit easily into numpy arrays. In this round you will learn how to train ANNs using large datasets (big data) that can only be processed sequentially since it cannot be stored into numpy arrays locally. Python provides convenient methods for handling such data. In particular, the Keras package includes DataGenerator objects. \n",
    "\n",
    "This notebook focuses on training and testing CNN module using data generator in Keras. Overall, the aim of this notebook is to get you familiar with :\n",
    "\n",
    "- data generator\n",
    "- learn to train and test model using data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python packages and libraries\n",
    "\n",
    "# provides functionality to train neural network\n",
    "import tensorflow as tf\n",
    "\n",
    "# provides mathematical functions to operate on arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# library to interact with operating system\n",
    "import os\n",
    "\n",
    "# function for randomization of data and labels \n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# function for computing prediction error \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# library for generating plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the data\n",
    "\n",
    "In this notebook,  we will use [Rock Paper Scissors Dataset](http://www.laurencemoroney.com/rock-paper-scissors-dataset/) generated by Laurence Moroney from Google. This dataset consists of images from a variety of different hands, from different races, ages and genders, posed into Rock / Paper or Scissors and labelled as such. The dataset consists of total of 2892 CGI generated images and divided into train, test and validation set. Each image is 300×300 pixels in 24-bit color.\n",
    "\n",
    "Each image has a name or id (e.g. 'paper03-088.png') and images are stored stored in '../../data/rps' directory as follows:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"dir_str.png\"\n",
    "         alt=\"\">\n",
    "    <figcaption style=\"text-align:center\" > Directory structure of dataset\n",
    "        \n",
    "</figure> \n",
    "\n",
    "\n",
    "As you can see, our main data directory is called 'rps'. Inside this directory (folder), there are 3 subdirectories named 'train', 'test', and 'validation' each of which contains train, test and validation data (images). Inside of each subdirectory there are 3 more subdirectories named 'rock', 'paper' and 'scissors', which contain images belonging to corresponding class labels (3 classes 'rock', 'paper' and 'scissors').\n",
    "\n",
    "In order to feed the images to CNN model we need to assign a numeric label to each image (e.g. all images in the folder 'paper' will be assigned to the numeric label 0 and all images in the folder 'rock' - to the numeric label 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of traditional machine learning methods in Python, e.g., those provided by the package `scikit-learn`, follows a similar workflow. First we load the training data into numpy arrays that store the features and labels of all training data points. These numpy arrays are then fed into some \".fit()\" function that implements the learning algorithm for a particular ML model. After the model has been trained we apply it to new data points, using some `.predict()` function, in order to obtain predictions for the labels of those new data points. \n",
    "\n",
    "Deep neural networks often need to be trained on extremely large training data sets which simply do not fit into a single numpy array. In such case, we need to find the way to train the model efficiently. One approach to overcome such scenario is to (1) divide dataset into smaller sets or parts called **batches**, (2) load each batch at time in a memory and (3) feed it to network. This process is repeated until we have trained the model by desired number of epochs. \n",
    "\n",
    "The total number of samples present in a single batch is called **batch size**.  The parameters (weight and bias) of network is updated after number of samples defined by a batch size is propagated through the network. Batch size is one of the hyperparameters of the deep neural network.\n",
    "\n",
    "To accomplish this task, Keras provides \n",
    "[ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) \n",
    " class to load and augment images in batches on-the-fly while training deep neural network. [Image augmentation](https://d2l.ai/chapter_computer-vision/image-augmentation.html) is a type of regularization technique which helps the model to avoid overfitting the training data. We will talk more about image augmentation in upcoming notebook.  \n",
    "With ImageDataGenerator class we can create an Python generator object, a function that behaves as an iterator and can be used e.g. in for loops. \n",
    "This class has following three methods to read the images either from directories or from numpy array. \n",
    "- .flow()\n",
    "\n",
    "takes data and label numpy arrays & generates batches of data\n",
    "\n",
    "- .flow_from_dataframe()\n",
    "\n",
    "takes the dataframe and the path to a directory & generates batches of data\n",
    "\n",
    "- .flow_from_directory() \n",
    "\n",
    "takes the path to a directory & generates batches of data\n",
    "\n",
    "Therefore, ImageDataGenerator class allows us to automatically generate batches of numpy arrays from image files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in more detail how Keras ImageDataGenerator class is working. Calling `ImageDataGenerator()` will create a Python generator function [link](https://wiki.python.org/moin/Generators) - function which allows you to declare a function that behaves like an iterator, i.e. it can be used in a for-loop. It looks like a normal function except that it contains `yield` expression (instead of `return` ) for producing a series of values usable in a for-loop or that can be retrieved one at a time with the `next()` function [link](https://docs.python.org/3/glossary.html#term-generator).\n",
    "\n",
    "Here is one example of a simple Python generator function, which generates infinite sequence of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a Python generator function, which can create an infinite sequence of numbers\n",
    "def sequence_generator():   \n",
    "    number = 0\n",
    "    while True:\n",
    "        # `yield` statement pauses the function, saves all its states\n",
    "        # and later continues from there on successive calls\n",
    "        yield number\n",
    "        number += 1\n",
    "\n",
    "numbers = []\n",
    "for number in sequence_generator():\n",
    "    numbers.append(number)\n",
    "    if number>9: # we need to include `break` statement, otherwise the for-loop will iterate infinitely\n",
    "        break\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can access generated values of Python generator by using `next()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable `a` is  <class 'generator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create generator object\n",
    "a = sequence_generator()\n",
    "# display the type of an object \n",
    "print(\"The variable `a` is \", type(a))\n",
    "\n",
    "# display values generated by generator object by using function `next()`\n",
    "next(a), next(a), next(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use Keras `ImageDataGenerator` class to create a Python generator function and use this function to create batches of Rock Paper Scissors Dataset.\n",
    "\n",
    "First, we need to create Python generator object by calling `ImageDataGenerator()`. There are many arguments for `ImageDataGenerator()` ([docs here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#arguments_17)), which allows image manipulation and augmentaion, but this time we will only use argument `rescale`, which rescale values in the range 0-255 to range 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The object `train_datagen` is a <class 'tensorflow.python.keras.preprocessing.image.ImageDataGenerator'>\n"
     ]
    }
   ],
   "source": [
    "# import ImageDataGenerator class\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create ImageDataGenerator object for training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) \n",
    "# create ImageDataGenerator object for validation data\n",
    "val_datagen = ImageDataGenerator(rescale=1./255) \n",
    "# create ImageDataGenerator object for validation data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "# print the type of the ImageDataGenerator object \n",
    "print(\"The object `train_datagen` is a\", type(train_datagen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we've created the generator, we need to choose how to acceess the data (images). As we discussed above, we don't want to load all our data into RAM at once, but rather access data one batch at a time. To do that, we will use `ImageDataGenerator.flow_from_directory()` method ([docs here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory)) to generate the image batches from given directory. This method takes the path to the target directory as an input and generates batches of numpy arrays from images. In addition, it automatically assignes lables to the data based on directory structure (each subdirectory will be given a label). E.g. if directory '../../data/rps/train' contains 3 subdirectories 'rock', 'paper' and 'scissors', this methods will assigned numeric labels to the image, dependent on image location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use following arguments with the `.flow_from_directory()` method ([docs here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory)).\n",
    "\n",
    "- `batch_size` - number of images per each batch\n",
    "\n",
    "- `directory` - path to our images\n",
    "\n",
    "- `shuffle` - randomize the order of images\n",
    "\n",
    "- `target size`  - the dimensions to which all images found will be resized\n",
    "\n",
    "- `class_mode=None` - do not return the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/rps/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-97ecd706acb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../../data/rps/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                     target_size=(100, 100))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# create generator function for validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m val_generator = val_datagen.flow_from_directory(batch_size=32,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         )\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/rps/train'"
     ]
    }
   ],
   "source": [
    "# create generator function for training dataset\n",
    "train_generator = train_datagen.flow_from_directory(batch_size=32,\n",
    "                                                    directory='../../data/rps/train',\n",
    "                                                    shuffle=True,\n",
    "                                                    target_size=(100, 100))\n",
    "# create generator function for validation dataset\n",
    "val_generator = val_datagen.flow_from_directory(batch_size=32,\n",
    "                                                directory='../../data/rps/validation',\n",
    "                                                shuffle=True,\n",
    "                                                target_size=(100, 100))\n",
    "\n",
    "# create generator function for testing dataset\n",
    "# for testing dataset we are using the batch size of 372 (all test images)\n",
    "# because we want generator to return the whole test dataset (only one batch)\n",
    "test_generator = test_datagen.flow_from_directory(batch_size=372,\n",
    "                                                directory='../../data/rps/test',\n",
    "                                                shuffle=True,\n",
    "                                                target_size=(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-445325144db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print the type of the ImageDataGenerator object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The object `train_generator` is a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# print the type of the ImageDataGenerator object \n",
    "print(\"The object `train_generator` is a\", type(train_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any Python generator we can access and inspect the values created by generator by using `next()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65983fe056e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# retrieve first values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_batch1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# display numpay arrays shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_batch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# retrieve first values\n",
    "image_batch1, label_batch1 = next(train_generator)\n",
    "\n",
    "# display numpay arrays shape\n",
    "image_batch1.shape, label_batch1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the generator returns two numpy arrays. First array contain pixel values of 32 100x100 RGB images. The second array contains one-hot encoded labels for each of 32 images:\n",
    "\n",
    "`class 'paper'    --> numeric label [1, 0, 0]`\n",
    "\n",
    "`class 'rock'     --> numeric label [0, 1, 0]`\n",
    "\n",
    "`class 'scissors' --> numeric label [0, 0, 1]`\n",
    "\n",
    "We can transform this vector label to the original form (where label is integer) with `np.argmax()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_batch1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8f0eb9f43834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-hot encoded label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_batch1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_batch1' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoded label\", label_batch1[2])\n",
    "\n",
    "print(\"Label: \", np.argmax(label_batch1[2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing images from \"Rock Paper Scissors\" dataset \n",
    "\n",
    "Now, when we created a generator objects, we can easily access any image and its label. In code snippet below we will plot some images and their labels from training datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_batch1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-101bfbc85e2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# read the image file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# format axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_batch1' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create figure object\n",
    "fig = plt.figure(1,figsize=(10,10))\n",
    "\n",
    "# list containing x axis labels\n",
    "class_name = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# plot first 9 images from the training dataset\n",
    "for i in range(9):\n",
    "    # create an empty subplot\n",
    "    plt.subplot(3,3,i+1)\n",
    "    # read the image file \n",
    "    image = plt.imshow(image_batch1[i])\n",
    "    \n",
    "    # format axes \n",
    "    plt.xticks([])  # remove x axis ticks        \n",
    "    plt.yticks([])  # remove y axis ticks\n",
    "    \n",
    "    # set the image x axis label\n",
    "    label = np.argmax(label_batch1[i])\n",
    "    plt.xlabel(class_name[label]+\" - \"+str(label), fontsize = 16)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN model \n",
    "\n",
    "Now, let's train the CNN model using the generators for train, validation and test datasets that we created. CNN will consist of the following layers:\n",
    "    \n",
    "Input → 3 * (Conv → Pool) → Flatten → Dense → Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 49, 49, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 47, 47, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 6,778,883\n",
      "Trainable params: 6,778,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the model architecture \n",
    "model = tf.keras.models.Sequential([   \n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "  \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    # flatten the results to feed to DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RMSprop optimizer\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy, \n",
    "              optimizer=RMSprop(lr=0.001), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, let's recap parameter **epochs** in `.fit method()`. \n",
    "**Epochs** parameter defines the number of times the model is trained over the ENTIRE dataset. \n",
    "\n",
    "Suppose, there are 1000 training samples. We divide the training samples into 5 batches of size 200 samples. We train the model using 200 samples, i.e first batch (from 1st to 200th) from the training dataset. Next, we take the second 200 samples (from 201st to 300th) and trains the network again. We can keep doing training until we have propagated all samples in training set through the network. When we train the model once, using ENTIRE dataset, we call it one epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB!** In earlier version of Keras, `.fit_generator()` method is used to train the model when using data generator as input. In the current version `.fit_generator()` function  IS DEPRECATED and will be removed in a future version.  Now, `.fit()` method supports generator as an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e0a4228158b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the CNN\n",
    "history = model.fit(train_generator, epochs=10,validation_data = val_generator, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-44e70ce67b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# for each training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#-----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0macc\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Retrieve results on training and validation data sets\n",
    "# for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc      = history.history['accuracy']\n",
    "val_acc  = history.history['val_accuracy']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs   = range(1,len(acc)+1) # get number of epochs\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "plt.plot(epochs, acc,  label='Training accuracy')\n",
    "plt.plot(epochs, val_acc,  label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlim(1, 6)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b08668729e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.spines.top'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "plt.plot(epochs, loss,  label='Training Loss')\n",
    "plt.plot(epochs, val_loss,  label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlim(1, 6)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can see that our model is over-fitting immediately after the first epoch. As we are training our model with relatively small training dataset (2520 training images), it is not surprising to see overfitting happening. In next notebook, we will try couple of techniques to overcome the problem of overfitting when training with a small dataset (image augmentaion). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for test images\n",
    "pred = model.predict(test_generator)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve labels from generator function\n",
    "test_image, test_label = next(test_generator)\n",
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(np.argmax(test_label,axis=1), np.argmax(pred,axis=1))\n",
    "print(f'The test set accuracy of model is {test_accuracy}')\n",
    "\n",
    "#compute total correct and incorrect prediction \n",
    "correct=np.nonzero(np.argmax(pred,axis=1)==np.argmax(test_label,axis=1))[0]\n",
    "incorrect=np.nonzero(np.argmax(pred,axis=1)!=np.argmax(test_label,axis=1))[0]\n",
    "\n",
    "print(\"Correct predicted labels:\",correct.shape[0])\n",
    "print(\"Incorrect predicted labels:\",incorrect.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we've showed how to create a data generator with Keras `ImageDataGenerator()` and use it for training and validation of the CNN model. The data generator function creates small batches of the data to sequentially feed these batches to neural network and train the model. Data generator is an effective way to train the model when you have large dataset and limited computation power. We covered one way of loading the image data from given directory with `.flow_from_directory()` method, but there are other methods to create batches of the data. In the next notebook we will see how we can use Keras `ImageDataGenerator` class to perform a real time data augmentation, which in turns helps to prevent an over-fitting of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional material \n",
    "\n",
    "### Blog posts\n",
    "- [Corey Schafer — Python Tutorial: Generators — How to use them and the benefits you receive](https://www.youtube.com/watch?v=bD05uGo_sVI)\n",
    "- [ImageDataGenerator – flow_from_directory method](https://theailearner.com/2019/07/06/imagedatagenerator-flow_from_directory-method/)\n",
    "- [ImageDataGenerator – flow_from_dataframe method](https://theailearner.com/2019/07/06/imagedatagenerator-flow_from_dataframe-method/)\n",
    "- [ImageDataGenerator – flow method](https://theailearner.com/2019/07/06/imagedatagenerator-flow-method/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
