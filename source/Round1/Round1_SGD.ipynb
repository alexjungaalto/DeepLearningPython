{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-EJ3311 - Deep Learning with Python, 09.09.2020-18.12.2020\n",
    "## Round 1 -   Gradient Based Optimization and Learning\n",
    "S. Abdurakhmanova and A. Jung\n",
    "\n",
    "Aalto University (Espoo, Finland) and Fitech.io (Finland) \n",
    "\n",
    "This notebook demonstrates a simple but powerful method to find an optimal choice for the weight vector $\\mathbf{w}$ of a predictor. The idea is to tune (adjust) the weight vector according to the gradient of the loss function. The loss function represents the average loss incurred over a training set for a particular weight vector. This average loss is also known as the **training error** of a predictor. By evaluating the training error for predictors with different weight vectors, we obtain an objective or **loss function** $f(\\mathbf{w})$. The loss function maps a weight vector to the training error incurred by the predictor map corresponding to that weight vector. \n",
    "\n",
    "We will detail a simple iterative algorithm which is called **gradient descent** (GD). Loosely speaking, GD finds an approximate minimizer of the training error by incrementally improving the current guess for the optimal weights by moving into the opposite gradient direction. We will also discuss a slight variation of GD known as **stochastic gradient descent** (SGD). SGD is one of the most widely used optimization methods within deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- understand how to learn a predictor map by minimizing a loss function (\"training error\")\n",
    "\n",
    "- understand how gradients can be used to tune (parameters of) predictor map to minimize loss function \n",
    "\n",
    "- understand motivation for and implementation of stochastic gradient descent SGD\n",
    "\n",
    "- understand SGD components \"batch\", \"batch size\", \"learning rate\" and \"epochs\"\n",
    "\n",
    "- be aware of some advanced variants of SGD such as ADAM or RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Reading\n",
    "\n",
    "-  Loss function [chapter 1.1.5](https://livebook.manning.com/book/deep-learning-with-python/chapter-1/49) of the book \"Deep Learning with Python\" by F. Chollet. \n",
    "-  Gradient-based optimization [chapter 2.4](https://livebook.manning.com/book/deep-learning-with-python/chapter-2/193) of the book \"Deep Learning with Python\" by F. Chollet. \n",
    "\n",
    "## Additional Material (Optional!)\n",
    "\n",
    "### Hypothesis Space and Loss Functions\n",
    "\n",
    "- Video on Hypothesis Space https://youtu.be/CDcRfak1Mh4\n",
    "- Video on Loss Function https://youtu.be/Uv9lihDfsBs\n",
    "- Chapter 2 and 3 of \"Machine Learning: Basic Principles\" https://arxiv.org/pdf/1805.05052.pdf\n",
    "\n",
    "### Derivatives and Gradients\n",
    "\n",
    "- https://www.mathsisfun.com/calculus/derivatives-introduction.html\n",
    "- (chapters 3-4) https://openstax.org/books/calculus-volume-1/pages/3-1-defining-the-derivative#27277\n",
    "\n",
    "### Stochastic Gradient  Descent\n",
    "\n",
    "- Andrew Ng, https://www.youtube.com/watch?v=F6GSRDoB-Cg \n",
    "- StatQuest, https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
    "- 3Blue1Brown, https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "- Chapter 5 of \"Machine Learning: Basic Principles\" https://arxiv.org/pdf/1805.05052.pdf\n",
    "- Optimization Chapter of Deep Learning Book https://www.deeplearningbook.org/contents/optimization.html\n",
    "\n",
    "### Variants of SGD\n",
    "\n",
    "- Adam, https://youtu.be/JXQT_vxqwIs\n",
    "- RMSprop, https://youtu.be/_e-LFe_igno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Every machine learning (ML) method involves a (more or less explicit) choice of a **hypothesis space**. The hypothesis space of a ML method is a subset of all possible **predictor maps**. A predictor map $h(\\mathbf{x})$ maps the feature vector $\\mathbf{x}$ of a data point to the predicted label $\\hat{y} = h(\\mathbf{x})$. The main goal of many ML methods is to find a good predictor map such that $\\hat{y} \\approx y$ for any data point. \n",
    "\n",
    "For computational reasons, it is impossible to consider all possible maps. There are already uncountable infinite number of maps that use a single real-valued feature $x \\in \\mathbb{R}$ and output a real-valued prediction $\\hat{y} \\in \\mathbb{R}$. Any ML method that runs on a computer with finite resources, can only use a subset of predictor maps which we refer to as the hypothesis space underlying that ML method. \n",
    "\n",
    "The hypothesis space is a design choice. Finding a good choice for a given dataset and computational infrastructure might be challenging and often requires some experience in applying different ML methods. Since current desktop computers are good at manipulating matrices and vectors, a very popular choice for a hypothesis space is the space \n",
    "of linear maps. Indeed, linear maps can be naturally represented by vectors and matrices. \n",
    "\n",
    "Let us consider data points characterized by $d$ features $x_{1},\\ldots,x_{d}$ and one real-valued label $y$. Here, a **linear predictor** is of the form  \n",
    "\n",
    "$$ h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{i=1}^{d} w_{i} x_{i}$$\n",
    "\n",
    "with some weights $w_{1},\\ldots,w_{d}$ which we stack into a weight (or parameter) vector $$\\mathbf{w} = \\big(w_{1},\\ldots,w_{d}\\big)^{T}.$$ A particular choice for the weight vector results in a particular predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. Finding a good predictor becomes equivalent to finding a weight vector such that the corresponding predictor $h^{(\\mathbf{w})}(\\mathbf{x})$ performs well. \n",
    "\n",
    "The focus of this course on a particular class of ML methods, i.e., deep learning methods. Deep learning methods use hypothesis spaces that are much larger than the space of linear maps. Similar to linear maps, the predictor maps used by deep learning methods are parametrized by a weight (or parameter) vector. The entries $w_{i}$ of the weight vector are the individual weights associated with the connections (links) of an artificial neural network (ANN). \n",
    "\n",
    "We will discuss how ANN are used to represent non-linear predictor maps in much detail within the upcoming Round 2 of this course. For the sake of this round, we only need to know that an ANN is just a convenient way to represent a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$ that uses the features of a data point as input and whose function value is the predicted label value. \n",
    "\n",
    "Similar to linear maps, the predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$ represented by an ANN depends on a weight vector $\\mathbf{w}$. In contrast to linear predictors, the predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$ obtained from an ANN is highly non-linear. However, the basic principle of learning a good predictor by adjusting the weight vector $\\mathbf{w}$ is the same for linear regression (linear predictors) and deep learning (maps represented by ANN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods aim at finding a good choice for the weights (and bias) of an **artificial neural network (ANN)**. To measure how \"good\" a particular choice for the weights is, we need to define a loss function. For a given pair of predicted label value $\\hat{y}$ and true label value $y$, the loss function $L(y,\\hat{y})$ provides a measure for the error, or \"loss\", incurred in predicting the true label $y$ by $\\hat{y}$. We emphasize that the precise definition of the loss function is a design choice. In principle, the deep learning engineer is free to choose an arbitrary loss function used to guide the training of the ANN. \n",
    "\n",
    "Some particular choices for the loss function have proven useful in many applications. If the label values are numeric (like a temperature or a weight), then the squared error loss $L(y,\\hat{y})=(y-\\hat{y})^2$ is often a good choice for the loss function. If the label values are categories (like \"cat\" and \"dog\"), we might use  the \"0/1\" loss $L(y,\\hat{y})=1$ if and only if $y=\\hat{y}$ and $L(y,\\hat{y})=0$ otherwise.\n",
    "\n",
    "Deep neural networks are fed with a large number of labeled data points \n",
    "\n",
    "$$\\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big).$$\n",
    "\n",
    "To measure the quality of a particular choice for the weights $\\mathbf{w}$ of the ANN, we first compute the resulting predictions $\\hat{y}^{(i)}$ obtained when feeding the feature vectors $\\mathbf{x}^{(i)}$ into the ANN. \n",
    "Then we calculate the average loss (or \"training error\")\n",
    "\n",
    "$$ (1/m) \\big( L(y^{(1)},\\hat{y}^{(1)})+L(y^{(2)},\\hat{y}^{(2)})+\\ldots+L(y^{(m)},\\hat{y}^{(m)}) \\big).$$\n",
    "\n",
    "Note that the training error depends on the weights $\\mathbf{w}$ of the ANN via the predictions $\\hat{y}^{(i)}$. Indeed, the predictions $\\hat{y}^{(i)}=h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$ are obtained by applying the ANN with weights $\\mathbf{w}$ to the input feature vector $\\mathbf{x}^{(i)}$. By evaluating the training error for different choices for the weights, we obtain a **cost or loss function** $f(\\mathbf{w})$ which guides the optimal choice for the weights $\\mathbf{w}$. \n",
    "\n",
    "The choice for the weights resulting in minimum training error are obtained by solving \n",
    "\n",
    "$$ \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w})$$\n",
    "\n",
    "With a slight abuse of notation, we refer to the function $f(\\mathbf{w})$ also as the loss function. For a given choice for the weight vector $\\mathbf{w}$, the loss function $f(\\mathbf{w})$ provides a quality measure for this choice. It should be clear from context if we mean by **loss function** the training error $f(\\mathbf{w})$ (which is a function of a weight vector) or the loss $L(y,\\hat{y})$ itself (which is a function of a pair of label values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error \n",
    "\n",
    "Maybe the most widely used loss function for applications involving numeric label values $y \\in \\mathbb{R}$ is the squared error loss \n",
    "\n",
    "$$L(y,\\hat{y}) = (\\underbrace{y- \\hat{y}}_{\\mbox{prediction error}})^{2}.$$\n",
    "\n",
    "We assess the quality of a predictor $\\hat{y} = h^{(\\mathbf{w})}(\\mathbf{x})$ by the average loss incurred over a set of labeled data points (the **training set**). For the squared error loss this average is referred to as the **mean squared error (MSE)** \n",
    "\n",
    "$$ f(\\mathbf{w}) = (1/m) \\big( \\big( y^{(1)}-\\hat{y}^{(1)}\\big)^{2}+\\big( y^{(2)}-\\hat{y}^{(2)}\\big)^{2}+\\ldots+\\big( y^{(m)}-\\hat{y}^{(m)}\\big)^{2} \\big). $$\n",
    "Note that the MSE on the right hands side depends on the weight vector $\\mathbf{w}$ via the predictions $\\hat{y}^{(i)}$ obtained by applying the predictor map $h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the loss $f(\\mathbf{w})$, viewed as a function of the weights $\\mathbf{w}$, depends on two components. First, it depends on how the predictor map depends on the weights. Second, it depends on the choice of the loss function $L(y,\\hat{y})$ used to measure the loss incurred by predicting the true label value $y$ with the prediction $\\hat{y}$. \n",
    "\n",
    "The combination of linear predictor functions and squared error loss $L(y,\\hat{y})=(y-\\hat{y})^{2}$ is a very popular as they result in a [convex](https://en.wikipedia.org/wiki/Convex_function) and [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) loss function $f(\\mathbf{w})$. A convex function has the attractive property that any local minimum is always also a [global minimum](https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg). If a convex function is also differentiable, it can be minimized by a simple but powerful algorithm which is known as **gradient descent**. \n",
    "\n",
    "<img src=\"MSELinPred.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods use predictor maps represented by ANN with tunable weights. In this case, the predictor depends non-linearly on the weights. As a result, we obtain (highly) non-convex loss landscapes. Below you can see examples of loss function landscapes of more complicated models (neural networks), which illustrates that finding a minimum of these loss functions is not a trivial task.\n",
    "\n",
    "<img src=\"NNloss.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\">image source</a></center>\n",
    "<center><a href=\"https://arxiv.org/abs/1712.09913/\">original paper</a></center>\n",
    "\n",
    "Here you can find more examples of visualizations for loss functions obtained from representing a predictor map using ANN:\n",
    "\n",
    "[3D visualization of NN loss functions](http://www.telesens.co/loss-landscape-viz/viewer.html) \\\n",
    "[3D animation of NN loss functions](https://www.youtube.com/watch?time_continue=32&v=aq3oA6jSGro&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "We will now introduce a simple algorithm that allows to find (good approximations to) the optimal weight vector $\\mathbf{w}_{\\rm opt}$ for a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. The optimum weight vector should result in the smallest possible loss  \n",
    "\n",
    "\\begin{align} \n",
    "f(\\mathbf{w}_{\\rm opt}) = \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w}) \\mbox{ with } f(\\mathbf{w})& = (1/m) \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)} \\big)^{2} \\nonumber \\\\ \n",
    "& =(1/m) \\sum_{i=1}^{m} (y^{(i)} - h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big) \\big)^{2}. \\end{align}\n",
    "\n",
    "**Gradient descent (GD)** constructs a sequence of weight vectors $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\ldots$ such that the loss values $f\\big(\\mathbf{w}^{(0)}\\big),f\\big(\\mathbf{w}^{(0)}\\big),\\ldots$ tends toward the minimum loss. GD is an iterative algorithm that gradually improves the current guess (approximation) $\\mathbf{w}^{(k)}$ for the optimum weight vector.  \n",
    "\n",
    "There are many different strategies for choosing the first (or initial) guess $\\mathbf{w}^{(0)}$. One simple approach is to choose the initial weights randomly. Given the current weight vector $\\mathbf{w}^{(k)}$, how does GD know in which \"direction\" to go to find a better weight vector $\\mathbf{w}^{(k+1)}$? Mathematics, or [calcululs](https://en.wikipedia.org/wiki/Differential_calculus) to be specific, tells us that this direction is precisely the opposite of the gradient $\\nabla f(\\mathbf{w})$. More precisely, for small step size, the steepest descent is towards the opposite direction of the [gradient](https://en.wikipedia.org/wiki/Derivative). We can think of [GD](https://en.wikipedia.org/wiki/Gradient_descent) as imitating a hiker who takes a sequence of (small) steps downhill.\n",
    "\n",
    "<img src=\"GradientHiker.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Given the downhill direction $- \\nabla f\\big(\\mathbf{w}^{(k)}\\big)$ at the current estimate $\\mathbf{w}^{(k)}$, we take a step \n",
    "\n",
    "$$\\mbox{(Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha}_{\\mbox{step size}} \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Here, we used a tuning parameter $\\alpha>0$ which adjusts the step size for the step downhill. We will refer to this parameter as **learning rate**. This name is due to the fact that choosing a larger value for $\\alpha$ tends to speed up the progress of GD to reach the optimum weight vector. Thus, increasing the value of $\\alpha$ tends to speed up the learning of a good weight vector for a predictor map $h^{(\\mathbf{w})}$. \n",
    "\n",
    "The GD algorithm requires the specification of a suitable learning rate  $\\alpha$ and initial guess $\\mathbf{w}^{(0)}$ and then repeating the gradient step for a sufficient number of iterations. One possible stopping criterion is to use a fixed number of iterations which might be dictated by constraints on processing duration we grant for GD (computing time costs money, [see here](https://aws.amazon.com/emr/pricing/)). \n",
    "\n",
    "Another option is to monitor the loss function and stop if consecutive iterates do not result in any significant decrease. Similarly, we could monitor the validation loss which is obtained by applying the predictor map using the current GD iterate $\\mathbf{w}^{(k)}$ to validation data which is different form the training data used to define the training loss. \n",
    "\n",
    "A key challenge in the use of GD is to find a good choice for the learning rate $\\alpha$. If the learning rate is too small (left plot below), the GD steps make too little progress and thus requires an excessive number of iterations to get close to the optimum weight vector. Conversely, if the learning rate is too high (right plot below), it is possible that GD iterates $\\mathbf{w}^{(k)}$ will \"overshoot\" the minimum and climb up the loss function on the other side of the minimum (GD diverges). \n",
    "\n",
    "<img src=\"lrate.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below visualizes how GD adapts the weights $\\mathbf{w}$ of a linear predictor $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to better fit the labeled data points (left) resulting in a smaller MSE (right). Note that after around $200$ iterations, gradient descent found weight vectors resulting in an almost minimum MSE. The additional iterations (beyond $200$) are (in some sense) a waste of computation as they do not decrease the MSE significantly. \n",
    "\n",
    "![SegmentLocal](plainGD.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain some intuition about the functioning of GD, let us work out the gradient update for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Here, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The gradient update of GD then becomes, in turn, \n",
    "\n",
    "\\begin{align} \n",
    "\\mathbf{w}^{(k+1)} & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big)\\nonumber \\\\ \n",
    " & = \\mathbf{w}^{(k)} + {\\alpha}\\,(2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big( y^{(i)} - \\hat{y}^{(i)} \\big) \n",
    ". \\end{align}\n",
    "\n",
    "Note that the gradient update involves the computation of the predictions $\\hat{y}^{(i)} = \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)}$. We will refer to this evaluation of the predictor map also as a **forward pass**. After the forward pass, the weight vector $\\mathbf{w}^{(k)}$ is updated by a weighted combination of the feature vectors $\\mathbf{x}^{(i)}$. The weight for the $i$th feature vector $\\mathbf{x}^{(i)}$ is given by the prediction error $ \\big( y^{(i)} - \\hat{y}^{(i)} \\big)$ incurred by the current weight vector for that data point. Thus, the gradient update puts more emphasis (larger weight) on those data points $\\big(\\mathbf{x}^{(i)},y^{(i)}\\big)$ which are not well predicted using the current weight vector $\\mathbf{w}^{(k)}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "\n",
    "Despite its conceptual simplicity, GD might not be practicable in ML applications involving massive amounts of data. Consider image classification where state-of-the art deep learning methods are trained on billions of images. The challenge in using GD for such big data applications is the computational complexity of computing the gradient $\\nabla f\\big( \\mathbf{w}^{(k)} \\big)$ of the loss function at the current estimate $\\mathbf{w}^{(k)}$. \n",
    "\n",
    "Let us have a closer look at the computation of the gradient for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. In this case, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The difficulty is that the summation involves all $m$ data points that form the training set. Thus, we might need to sum over billions of data points. Moreover, the data points might be stored decentralized all over the internet (in the \"cloud\"). A single iteration of GD might then simply take too long.\n",
    "\n",
    "In order to avoid the computational burden of computing the gradient, stochastic GD (SGD) approximates the gradient \n",
    "by using only a small subset (a \"batch\") of training data points. SGD is obtained from GD by replacing the exact gradient step by a noisy gradient update: \n",
    "\n",
    "$$\\mbox{(Noisy Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha^{(k)}}_{\\mbox{step size}}\\mathbf{g}^{(k)} \\mbox{ with } \\mathbf{g}^{(k)} \\approx \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "Note that we now use a varying step-size $\\alpha^{(k)}$ that changes along the iterations. This is necessary in order to attenuate the noise in the gradient estimate $\\mathbf{g}^{(k)}$. \n",
    "\n",
    "The most basic variant of SGD uses a single randomly chosen data point for computing the gradient estimate $\\mathbf{g}^{(k)}$. For the special case of linear predictor maps, we obtain \n",
    "$$ \\mathbf{g}^{(k)}   = -2 \\mathbf{x}^{(I)} \\big(y^{(I)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(I)} \\big) \\big).$$\n",
    "\n",
    "Note that the index $I$ of the data point is chosen randomly and independently for each new iteration $k$. Comparing this gradient estimate with the above formula for the exact gradient, we see that the SGD iteration does not require any summation over the training set. For large training sets this might yield a significant reduction in computational requirements for SGD compared to GD. \n",
    "\n",
    "Plain SGD and GD can be interpreted as special cases of **mini-batch SGD**. Mini-batch SGD does not use a single randomly chosen data point to compute the gradient estimate but rather uses several randomly chosen data points that form a batch $\\mathcal{S} = \\big\\{ \\big(\\mathbf{x}^{(i_{1})},y^{(i_{1})} \\big),\\ldots,\\big(\\mathbf{x}^{(i_{S})},y^{(i_{S})} \\big) \\big\\}$ of size $S$. For linear predictor maps, the gradient estimate is computed using the batch via \n",
    "$$ \\mathbf{g}^{(k)}   = -2 \\sum_{\\big(\\mathbf{x},y\\big) \\in \\mathcal{S}} \\mathbf{x} \\big(y - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x} \\big) \\big).$$\n",
    "\n",
    "Each iteration of mini-batch SGD uses a different batch of $S$ different data points. A sequence of iterations that uses each data point in one of the batches is referred to as one **epoch**. For example, if the training data set consists of $100$ data points and we use a batch size of $S=10$, then one epoch requires on average $100/10 = 10$ iterations of mini-batch SGD. \n",
    "\n",
    "Important special cases of mini-batch SGD are obtained for certain choices of the **batch size**: \n",
    "\n",
    "- GD (batch size = size of dataset)\n",
    "- Mini-batch SGD (1 < batch size < whole dataset)\n",
    "- plain SGD (batch size = 1)\n",
    "\n",
    "Note that the computational complexity of one iteration of mini-batch GD depends only on the batch size. Thus, \n",
    "for a fixed batch size (e.g., $S=128$) the complexity (runtime) of a single iteration becomes independent of the total number of training data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning and deep learning Python libraries, such as `sklearn` and `keras`, provide ready-to-use gradient-based optimization algorithms. However, it is instructive to implement our own simplified mini-batch stochastic gradient descent (or mini-batch SGD) algorithm for learning purposes. In this simple case we have 100 data points (samples) which are described by only one feature `X` and have labels stored in `y`. \\\n",
    "We need to find the optimal linear predictor `y_pred = weight*X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below imports the Python libraries whose functions we will use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                 # library for numerical arrays (vectors, matrices, tensors)\n",
    "import matplotlib.pyplot as plt                    # library providing tools for plotting data \n",
    "from sklearn import preprocessing                  # function for pre-processing input data\n",
    "from sklearn.linear_model import LinearRegression  # sklearn class for fitting linear predictor \n",
    "from sklearn.datasets import make_regression       # function to generate a random regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below uses the sklearn.dataset `make_regression` function to generate data points with features and labels. The feature and label values are obtained from random generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset for regression problem\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=2) \n",
    "X = X.reshape(-1,)\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used sklearn `preprocessing` module ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn-preprocessing-scale)) to scale our features `X`. Learn [here](https://www.youtube.com/watch?v=r5E2X1JdHAU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=20), why it is useful to normalize the data when applying the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the SGD algorithm, we need to write a helper function `batch()` that divides the dataset into small batches. We will provide the feature matrix `X` and label vector `y` as an input to the function. We also need to define the parameter `batch_size` which is the number of data points used for a single batch. The function `batch()` is a [Python generator function](https://docs.python.org/3/howto/functional.html#generators), meaning that the function can be used in for-loops and will return batches sequentially, one-by-one. Before splitting the dataset into batches, we will randomly shuffle the data. This ensures that every time we call `batch()` we obtain a batch having the same statistical properties. Loosely speaking, by shuffling the dataset before selecting the batch makes the individual data points independent and identically distributed (\"i.i.d.\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X,y,batch_size):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This is a function for creating mini-batches of the dataset.\n",
    "    The `yield` statement suspends function’s execution and sends \n",
    "    a value back to the caller, but retains enough state to enable \n",
    "    function to resume where it is left off. \n",
    "    \n",
    "    '''\n",
    "    # check if the number of data points is equal in feature matrix X and label vector y\n",
    "    # if assertion fails return error message \"Number of data points are different in X and y\"\n",
    "    assert X.shape[0] == y.shape[0], \"Number of datapoints are different in X and y\"\n",
    "    \n",
    "    # shuffle data points \n",
    "    # permutation will randomly re-arrange the order of the numbers\n",
    "    # which will be used as indices to create X and y with data points in different order\n",
    "    p = np.random.permutation(len(y))\n",
    "    X_perm = X[p] \n",
    "    y_perm = y[p]\n",
    "    \n",
    "    # generate batches\n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "        yield (X_perm[i:i + batch_size], y_perm[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below defines a function `gradient_step()` which implements a single gradient step. This function has as its input parameters the feature matrix `X`, the label vector `y`, the initial guess for the weights and a learning rate. \n",
    "\n",
    "The function `gradient_step()` implements the following steps:\n",
    "\n",
    "1. compute predictions for the data points in a batch, given the current weights\n",
    "2. compute MSE loss \n",
    "3. compute gradient of the loss function\n",
    "4. update the weights - change the weights values to the opposite direction from gradient `weight = weight - lrate*gradient`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(X,y,weight,lrate):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This is a function for performing plain gradient descent algorithm with MSE loss function.\n",
    "\n",
    "    squared error loss for a single data point:        \n",
    "      loss = (y - weight*x)**2\n",
    "    derivative w.r.t. to weight for a single data point:  \n",
    "      der_w = -2x*(y - weight*x)\n",
    "      \n",
    "    To compute the MSE and derivative of MSE - take an average of all data points.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # performing Gradient Step:\n",
    "    \n",
    "    # 1. compute predictions, given the weight\n",
    "    y_hat = weight*X   \n",
    "    # 2. compute MSE loss\n",
    "    MSE = np.mean((y - y_hat)**2)\n",
    "    # 3. compute gradient of loss function\n",
    "    der_w = -2*np.mean(X*(y - y_hat))  \n",
    "    # 4. update the weights\n",
    "    weight = weight - lrate* der_w            \n",
    " \n",
    "    return weight, MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine all elements in `minibatchSGD()` function. The parameters of the function: \\\n",
    "`X` - features \\\n",
    "`y` - labels   \\\n",
    "`batch_size` - number of samples per batch \\\n",
    "`epoches` - how many times to iterate through the ENTIRE dataset \\\n",
    "`lrate` - step size or learning rate of gradient descent\n",
    "\n",
    "The function will return learnt weights after running the algorithm through entire dataset epoches times.\n",
    "In addition, the function will return list `loss` where the MSE loss values for all batches and epoches are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def minibatchSGD(X,y,batch_size,epochs,lrate):  \n",
    "    \n",
    "    # initialize weight randomly\n",
    "    weight = np.random.rand()    \n",
    "    # create list to store the loss values \n",
    "    loss = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run gradient step for each batch\n",
    "        for X_batch,y_batch in batch(X,y,batch_size):\n",
    "            weight, MSE = gradient_step(X_batch,y_batch,weight,lrate)\n",
    "            # store MSE loss of each batch of each epoch\n",
    "            loss.append(MSE)\n",
    "                       \n",
    "        # one epoch is finished when the algorithm goes through ALL batches\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our SGD implementation and run the algorithm for:\n",
    "\n",
    "- batch sizes = 1 (one data point) **(\"true\" SGD)**\n",
    "- batch sizes = 10 **(mini-batch SGD)**\n",
    "- batch sizes = 100 (entire dataset) **(Batch GD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 200\n",
    "lrate = 0.1\n",
    "\n",
    "# batch size 1\n",
    "batch_size1 = minibatchSGD(X,y,1,epochs,lrate)\n",
    "# batch size 10\n",
    "batch_size10 = minibatchSGD(X,y,10,epochs,lrate)\n",
    "# batch size 100\n",
    "batch_size100 = minibatchSGD(X,y,100,epochs,lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `minibatchSGD()` returns weight and loss incurred during the training. Let's retrieve the loss values for plotting and print out learnt weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  200\n",
      "Iterations per epoch: 100, 10, 1,\n",
      "Total number of iterations:  20000 2000 200\n",
      "\n",
      "Weights:\n",
      "\n",
      "SGD with batch size = 1 results in weight w = 43.63                 \n",
      "SGD with batch size = 10 results in weight w = 60.68                 \n",
      "SGD with batch size = 100 results in weight w = 60.27\n"
     ]
    }
   ],
   "source": [
    "# history of the MSE loss inccured during learning\n",
    "batch_size1_loss   = batch_size1[-1]\n",
    "batch_size10_loss  = batch_size10[-1]\n",
    "batch_size100_loss = batch_size100[-1]\n",
    "\n",
    "# let's check that length of list `loss` is equal to\n",
    "# X.shape[0]/batch_size*epochs\n",
    "print(\"Epochs: \", epochs)\n",
    "print(\"Iterations per epoch: {:.0f}, {:.0f}, {:.0f},\".format(X.shape[0]/1, X.shape[0]/10, X.shape[0]/100))\n",
    "print(\"Total number of iterations: \",len(batch_size1_loss), len(batch_size10_loss), len(batch_size100_loss))\n",
    "\n",
    "# display weights learnt during the SGD\n",
    "print(\"\\nWeights:\\n\\nSGD with batch size = 1 results in weight w = {:.2f}\\\n",
    "                 \\nSGD with batch size = 10 results in weight w = {:.2f}\\\n",
    "                 \\nSGD with batch size = 100 results in weight w = {:.2f}\".format(\n",
    "                  batch_size1[0],batch_size10[0], batch_size100[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the optimal weights learnt by our algorithm and the optimal weight calculated by sklearn `LinearRegression()` class. This class does not use iterative gradient-based algorithms, but rather calculating optimal weight analytically [learn more here](https://www.youtube.com/watch?v=B-Ks01zR4HY).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.27113167581487"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create linear regression model \n",
    "reg = LinearRegression(fit_intercept=False) \n",
    "# fit a linear regression model \n",
    "reg = reg.fit(X.reshape(-1,1), y)\n",
    "# print optimal coefficients\n",
    "reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our simple GD algorithm does not deviate too much from the `sklearn` implementation of linear regression. Note that the deviations are larger for smaller batch size. This makes sense, as the gradient estimates are more accurate when we use more datapoints in a batch. \\\n",
    "It is useful to plot the loss values incurred during the training or learning. Let us plot the loss values for first 100 iterations (for first 100 batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create figure and axes objects\n",
    "# there will be 3 subplots in one row, y-axis is shared between subplots\n",
    "fig, axes = plt.subplots(1,3, sharey=True, figsize=(15,5))\n",
    "\n",
    "# create lists of loss values and batch sizes for further iteration in for-loop\n",
    "batch_loss_list = [batch_size1_loss, batch_size10_loss, batch_size100_loss]\n",
    "batch_size      = [1,10,100] \n",
    "\n",
    "for ax, batch_loss, size in zip(axes, batch_loss_list, batch_size):\n",
    "    # plot only first 100 values\n",
    "    ax.plot(np.arange(len(batch_loss[:100])), batch_loss[:100])\n",
    "    # remove top and right subplot's frames \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    # set subplot's title\n",
    "    ax.set_title(\"batch size = \"+str(size), fontsize=18)\n",
    "\n",
    "# set x- and y-axis labels\n",
    "axes[0].set_xlabel('batch #', fontsize=18)\n",
    "axes[0].set_ylabel('Loss', fontsize=18)\n",
    "\n",
    "# display figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure indicates that when using SGD with a small batch size, the loss is not decreasing monotonically but somewhat randomly fluctuating around a long-term decreasing trend. This happens because the weight updates use \"noisy\" estimates of the gradient. The noisy estimate is calculated by an averaging process using the data points in the mini-batch. The smaller the mini-batch size, the fewer data points we use for computing the average. Thus, the gradient noise becomes stronger with smaller batch size. In order avoid the accumulation of the gradient noise while running SGD updates the learning rate needs to be gradually decreased, e.g. by using diminishing learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the animations illustrating the training process with SGD where batch size is all data points (upper panel) and where batch size is 10 data points (lower panel, the current batch marked with red colour). In line with loss plots we created above, mini-batch SGD is more noisy than batch GD. Although, it seems that using plain batch GD is faster way to reach the minimum of the loss function, in practice when working with large datasets and thousands parameters (weights) in neural networks, this approach will be slower and more computationally expensive than mini-batch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration of SGD iterations for batch size = 100 (batch covers entire dataset)** \n",
    "![SegmentLocal](minibatchGD1.gif \"segment\")\n",
    "**Illustration of SGD iterations for batch size = 10 datapoints**\n",
    "![SegmentLocal](minibatchGD2.gif \"segment\")\n",
    "During each iteration of SGD, 10 data points are randomly selected to constitute a batch. This batch is used \n",
    "to compute the gradient estimate. The data points in the batch are shown in red. Note that during \n",
    "each iteration, a different set of 10 data points is chosen for the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Gradient-Based Optimization Algorithms\n",
    "\n",
    "Besides plain GD or mini-batch SGD, many deep learning methods use somewhat more advanced variants of gradient-based algorithms or optimizers ([list of optimizers available in deep learning Python library Keras](https://keras.io/api/optimizers/)). Some of the most known are SGD with momentum, RMSprop and Adam. \n",
    "\n",
    "Much like GD and SGD, these algorithms use gradients of the loss function $f(\\mathbf{w})$ to find weights $\\mathbf{w}$ such that the predictor $h^{(\\mathbf{w})}$ achieves (nearly) minimum loss. These variants differ in how they use (or \"interpret\") the gradient information to find the fastest route towards the minimum. In some cases these variants can find good weight vectors significantly faster (using fewer iterations) compared to mini-batch SGD. \n",
    "\n",
    "The animation below compares the \"routes\" taken by different optimizers to find a minimum of the [six-hump camel](https://www.sfu.ca/~ssurjano/camel6.html) function. \n",
    "\n",
    "![SegmentLocal](camel3D.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We have discussed the basic idea of using gradients of loss functions to iteratively improve the parameter values (weights) in a predictor map. Gradient based methods such as SGD and its variants turn out to be the perfect tool for training the deep neural networks in several aspects. First, somewhat surprisingly, SGD quickly finds weights for an ANN such that it performs well on new data points which are different from the training data. Moreover, mini-batch SGD requires only to have enough working memory (\"RAM\") to store the current batch (subset) of training data points instead of the entire dataset (which might be billions of high-resolution images). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
